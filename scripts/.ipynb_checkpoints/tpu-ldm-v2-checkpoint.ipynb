{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "358dac34-37a0-4022-a855-19639b42484e",
    "_uuid": "a8dcc00c-abc5-480f-8db6-df2253b334ed"
   },
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:01:47.796246Z",
     "iopub.status.busy": "2023-09-16T15:01:47.795570Z",
     "iopub.status.idle": "2023-09-16T15:02:00.334071Z",
     "shell.execute_reply": "2023-09-16T15:02:00.332772Z",
     "shell.execute_reply.started": "2023-09-16T15:01:47.796210Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qq protobuf==3.20.3 # required for tensorflow for some reason\n",
    "!pip install -Uqq tensorboard-plugin-profile cloud-tpu-profiler lpips transformers tfrecord wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:00.336376Z",
     "iopub.status.busy": "2023-09-16T15:02:00.336081Z",
     "iopub.status.idle": "2023-09-16T15:02:05.692508Z",
     "shell.execute_reply": "2023-09-16T15:02:05.691210Z",
     "shell.execute_reply.started": "2023-09-16T15:02:00.336348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download Ngrok to tunnel the tensorboard port to an external port\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip\n",
    "!rm *.zip\n",
    "!./ngrok authtoken <NGROK_AUTHTOKEN HERE> # you have to make a free NGrok account to get this token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:05.694319Z",
     "iopub.status.busy": "2023-09-16T15:02:05.694006Z",
     "iopub.status.idle": "2023-09-16T15:02:05.933059Z",
     "shell.execute_reply": "2023-09-16T15:02:05.931012Z",
     "shell.execute_reply.started": "2023-09-16T15:02:05.694281Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, gc; gc.collect()\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(processes = 10)\n",
    "results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n",
    "                        for cmd in [\n",
    "                        f\"tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006 &\",\n",
    "                        \"./ngrok http 6006 &\"\n",
    "                        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2222a0be-9b89-40f7-ae3e-05cb6b3c1a83",
    "_uuid": "34932f2f-da03-4133-b76b-b231d4c07e99",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:05.935813Z",
     "iopub.status.busy": "2023-09-16T15:02:05.935439Z",
     "iopub.status.idle": "2023-09-16T15:02:09.831055Z",
     "shell.execute_reply": "2023-09-16T15:02:09.829944Z",
     "shell.execute_reply.started": "2023-09-16T15:02:05.935770Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, warnings, logging\n",
    "os.environ['XLA_DOWNCAST_BF16'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# required for using multiprocessing for some reason - https://github.com/pytorch/xla/blob/master/contrib/kaggle/distributed-pytorch-xla-basics-with-pjrt.ipynb\n",
    "for env_var in ['TPU_PROCESS_ADDRESSES', 'CLOUD_TPU_TASK_ID', 'XRT_TPU_CONFIG']:\n",
    "    try:\n",
    "        os.environ.pop(env_var)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.experimental.pjrt_backend\n",
    "import torch_xla.experimental.pjrt as pjrt\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.debug.profiler as xp\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "from tfrecord.torch.dataset import MultiTFRecordDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tfrecord.tools import tfrecord2idx\n",
    "from torchvision.io import decode_jpeg\n",
    "from transformers import logging\n",
    "import torch.distributed as dist\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from textwrap import wrap\n",
    "from PIL import Image\n",
    "import torch.multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch, glob, gc, wandb, math\n",
    "\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    ENABLE_FLASH_ATTN = ('DISABLE_FLASH' not in os.environ) or (os.environ['DISABLE_FLASH'] != '1')\n",
    "except:\n",
    "    ENABLE_FLASH_ATTN = False\n",
    "\n",
    "DEBUG = False\n",
    "USE_WANDB = True\n",
    "ONE_CORE = False\n",
    "\n",
    "if USE_WANDB:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wab_key = user_secrets.get_secret(\"wab_key\")\n",
    "\n",
    "    os.environ['WANDB_API_KEY'] = wab_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:09.834266Z",
     "iopub.status.busy": "2023-09-16T15:02:09.833952Z",
     "iopub.status.idle": "2023-09-16T15:02:10.907775Z",
     "shell.execute_reply": "2023-09-16T15:02:10.906561Z",
     "shell.execute_reply.started": "2023-09-16T15:02:09.834234Z"
    }
   },
   "outputs": [],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "# to see profiling, capture a profile from Tensorboard with IP name = localhost:6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29367e32-0306-42ba-842a-62f1f53d6071",
    "_uuid": "5559374d-efaa-4e17-a740-acc9923e1cbf"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25d05c63-6c2e-4784-8bc5-5c708ad8d8f0",
    "_uuid": "d00c97c1-5ff7-452e-a6a5-a6bfcf96d8c0",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:10.909751Z",
     "iopub.status.busy": "2023-09-16T15:02:10.909402Z",
     "iopub.status.idle": "2023-09-16T15:02:10.922564Z",
     "shell.execute_reply": "2023-09-16T15:02:10.921611Z",
     "shell.execute_reply.started": "2023-09-16T15:02:10.909713Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1250\n",
    "STEPS_PER_EPOCH = 2048\n",
    "CROP_SIZE = 256\n",
    "BATCH_SIZE = 128 # per device batch size\n",
    "STEPS_PER_LOG = 32\n",
    "    \n",
    "TFREC_BASE_DIR = INDEX_BASE_DIR = \"/kaggle/input\" # where all tfrecord files are found\n",
    "CHILD_PATTERN = \"laion*\" # portion of tfrecord filename between the base dirs and the extension which will be found in the tfrecord and tfindex filename\n",
    "\n",
    "WANDB_RUN_PATH = WANDB_MODEL_FNAME = WANDB_VAE_RUN_PATH = WANDB_VAE_MODEL_FNAME = None\n",
    "#WANDB_RUN_PATH = 'tiewa_enguin/tpu_ldm_ddpm_v2/s198u8lu'\n",
    "#WANDB_MODEL_FNAME = 'ddpm_0036.pth'\n",
    "WANDB_VAE_RUN_PATH = 'tiewa_enguin/tpu_ldm_vae/nwllbf9z'\n",
    "WANDB_VAE_MODEL_FNAME = 'vae_0029_stable_norm.pth'\n",
    "CLIP_VERSION = 'openai/clip-vit-large-patch14'\n",
    "MAX_TOKENS = 64\n",
    "\n",
    "CKPT_DIR = '/kaggle/working'\n",
    "# save dirs may be different from load dirs if load dirs are unwritable (e.g. /kaggle/input)\n",
    "CKPT_SAVE_DIR = '/kaggle/working/checkpoints'\n",
    "\n",
    "#LEARNING_RATE = 1.5e-7 * BATCH_SIZE * 8\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_WARMUP_STEPS = 10000\n",
    "\n",
    "VAE_NC = 128\n",
    "VAE_CH_MULTS = [1, 2, 2, 4]\n",
    "VAE_NLAYERS_PER_RES = 2\n",
    "VAE_NZ = DDPM_NZ = 4\n",
    "\n",
    "DDPM_NC = 256\n",
    "DDPM_CH_MULTS = [1, 2, 4]\n",
    "ATTN_RESOLUTIONS = [0, 1, 2]\n",
    "DDPM_NLAYERS_PER_RES = 2\n",
    "\n",
    "if DEBUG:\n",
    "    STEPS_PER_EPOCH = 128\n",
    "    BATCH_SIZE = 32\n",
    "    STEPS_PER_LOG = 4\n",
    "    CLIP_VERSION = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "    VAE_NC = 32\n",
    "    VAE_CH_MULTS = [1, 1, 1, 1]\n",
    "    VAE_NLAYERS_PER_RES = 1\n",
    "    VAE_NZ = DDPM_NZ = 4\n",
    "    \n",
    "    DDPM_NC = 32\n",
    "    DDPM_CH_MULTS = [1, 1, 1]\n",
    "    ATTN_RESOLUTIONS = [0, 0, 0]\n",
    "    DDPM_NLAYERS_PER_RES = 1\n",
    "\n",
    "TEXT_CONTEXT_DIM = 768 if 'large' in CLIP_VERSION else 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6673347-0dc8-4a5e-9e05-fdd4e2ad4e27",
    "_uuid": "a1215cc3-45cb-453e-9d1e-cfcbd99cbb44"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:10.924069Z",
     "iopub.status.busy": "2023-09-16T15:02:10.923757Z",
     "iopub.status.idle": "2023-09-16T15:02:12.986550Z",
     "shell.execute_reply": "2023-09-16T15:02:12.985643Z",
     "shell.execute_reply.started": "2023-09-16T15:02:10.924041Z"
    }
   },
   "outputs": [],
   "source": [
    "join = os.path.join\n",
    "\n",
    "INDEX_PATTERN = join(INDEX_BASE_DIR, \"{}.tfindex\")\n",
    "ff_unpruned = glob.glob(join(TFREC_BASE_DIR, CHILD_PATTERN, '*.tfrecord'))\n",
    "ff = []\n",
    "for f in ff_unpruned:\n",
    "    if os.path.getsize(f) > 100e6: # greater than 100 MB:\n",
    "        ff.append(f)\n",
    "n_files = len(ff)\n",
    "\n",
    "FF_CHILD = []\n",
    "for f in ff:\n",
    "    child = os.path.splitext(f[len(TFREC_BASE_DIR)+1:])[0] # get rid of base dir and ext\n",
    "    FF_CHILD.append(child) # child is the common part of filename between tfrecord and tfindex file\n",
    "\n",
    "TFREC_PATTERN = join(TFREC_BASE_DIR, '{}.tfrecord')\n",
    "\n",
    "def process_ex(example):\n",
    "    bytes_arr = torch.tensor(np.frombuffer(example['jpg'], dtype=np.uint8))\n",
    "    image = decode_jpeg(bytes_arr).float() / 127.5 - 1.0\n",
    "    if image.shape[0] == 1:\n",
    "        image = image.repeat([3, 1, 1])\n",
    "    caption = example['txt'].decode('utf-8')\n",
    "    return image, caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1725a14a-15ef-4bf8-b4fb-9cb858929ac1",
    "_uuid": "3b932802-8de9-4bc6-84df-88b291236321"
   },
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5bb24465-aba6-4057-ad8e-d02644d611ed",
    "_uuid": "a475f5aa-dc0a-4f0c-b37e-bfa25d4d733f"
   },
   "source": [
    "Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b00aa13a-fde3-47a5-a353-f29e3124be68",
    "_uuid": "889901f6-6a7d-460e-95ad-b43582063a46",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:12.988116Z",
     "iopub.status.busy": "2023-09-16T15:02:12.987795Z",
     "iopub.status.idle": "2023-09-16T15:02:13.004625Z",
     "shell.execute_reply": "2023-09-16T15:02:13.003770Z",
     "shell.execute_reply.started": "2023-09-16T15:02:12.988088Z"
    }
   },
   "outputs": [],
   "source": [
    "def zero_module(module):\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "class StableNorm(nn.Module): # runs GroupNorm in FP32 because of bfloat16 stability issues when x is large but with small variance (i.e. x = 100) \n",
    "    def __init__(self, num_groups: int, num_channels: int):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(num_groups, num_channels).double()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.norm(x.double()).float()\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_c: int, nc: int, temb_c: int = None):\n",
    "        '''\n",
    "        in_c: number of input channels\n",
    "        nc: number of output channels\n",
    "        temb_c: number of t (time?) embedding input channels (or None if no time embedding)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.norm1 = StableNorm(32, in_c)\n",
    "        self.act1 = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_c, nc, 3, padding=1)\n",
    "        self.norm2 = StableNorm(32, nc)\n",
    "        self.act2 = nn.SiLU()\n",
    "        self.conv2 = zero_module(nn.Conv2d(nc, nc, 3, padding=1))\n",
    "        if temb_c is not None:\n",
    "            self.temb_proj = nn.Linear(temb_c, nc)\n",
    "        self.skip = nn.Conv2d(in_c, nc, 1, bias=False) if in_c != nc else None\n",
    "    \n",
    "    def forward(self, x, temb=None): # temb = t (time) embedding\n",
    "        skip = x if self.skip is None else self.skip(x)\n",
    "        x = self.conv1(self.act1(self.norm1(x)))\n",
    "        if temb is not None:\n",
    "            x = x + self.temb_proj(F.silu(temb))[:, :, None, None]\n",
    "        x = self.conv2(self.act2(self.norm2(x)))\n",
    "\n",
    "        return x + skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82650b58-adf3-46b4-9cb5-256750ceca5f",
    "_uuid": "8e173e61-f105-4b89-8050-5d9c361f3ca5"
   },
   "source": [
    "Down/Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3742533-c19b-42d1-8975-b1d31f3f7e94",
    "_uuid": "c884a649-5566-4b17-b9cb-8a066706c03e",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.006230Z",
     "iopub.status.busy": "2023-09-16T15:02:13.005841Z",
     "iopub.status.idle": "2023-09-16T15:02:13.023804Z",
     "shell.execute_reply": "2023-09-16T15:02:13.023007Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.006203Z"
    }
   },
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, nc: int):\n",
    "        '''\n",
    "        nc: number of input and output channels\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(nn.ZeroPad2d((0, 1, 0, 1)), nn.Conv2d(nc, nc, 3, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, nc: int):\n",
    "        '''\n",
    "        nc: number of input and output channels\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(nc, nc, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _B, _C, H, W = x.shape\n",
    "        return self.conv(F.interpolate(x, size=(H*2, W*2), mode='nearest')) # specifying scale factor offloads op to CPU: https://github.com/pytorch/xla/issues/2588"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3379575-24d6-4196-b6a2-e8a5f39ad64b",
    "_uuid": "17996295-dab0-461d-8c94-fb48fc7b0827"
   },
   "source": [
    "2D Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fbf5e464-dab6-4cfb-8778-4e5c5021d4e3",
    "_uuid": "c6f88362-6a10-49d3-9ae7-7f8237231c5e",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.025271Z",
     "iopub.status.busy": "2023-09-16T15:02:13.024980Z",
     "iopub.status.idle": "2023-09-16T15:02:13.046494Z",
     "shell.execute_reply": "2023-09-16T15:02:13.045630Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.025246Z"
    }
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module): # slightly faster and less mem than torch multihead attn (I suppose from QKV projection being fused)\n",
    "    def __init__(self, nc: int, nh: int, kv_dim: int = None, zero_last_layer: bool = True):\n",
    "        '''\n",
    "        nc: number of input and output channels\n",
    "        nh: number of heads (note: d_head = nc // nh)\n",
    "        kv_dim: dimensionality of key & value input (used for conditioning input in cross-attention; self-attention if kv_dim is None)\n",
    "        zero_last_layer: whether or not to zero-init the weights of the last layer (this helps out optimization of residual connections)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.nh = nh\n",
    "        self.dhead = nc // nh\n",
    "        \n",
    "        kv_dim = nc if kv_dim is None else kv_dim\n",
    "        self.q_in = nn.Linear(nc, nc, bias=False)\n",
    "        self.k_in = nn.Linear(kv_dim, nc, bias=False)\n",
    "        self.v_in = nn.Linear(kv_dim, nc, bias=False)\n",
    "        self.out = nn.Linear(nc, nc, bias=False)\n",
    "        if zero_last_layer:\n",
    "            self.out = zero_module(self.out)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        B, L, E = x.shape\n",
    "        if ENABLE_FLASH_ATTN:\n",
    "            return x.reshape(B, L, self.nh, self.dhead) # M N (H D) -> M N H D, D=self.dhead, H=self.nh)\n",
    "        return x.reshape(B, L, self.nh, self.dhead).permute(0, 2, 1, 3).contiguous() # M N (H D) -> M H N D, D=self.dhead, H=self.nh\n",
    "    \n",
    "    def forward(self, q, kv=None):\n",
    "        B, L, E = q.shape\n",
    "        if kv is None:\n",
    "            q, k, v = map(self.split_heads, (self.q_in(q), self.k_in(q), self.v_in(q)))\n",
    "        else:\n",
    "            q, k, v = map(self.split_heads, (self.q_in(q), self.k_in(kv), self.v_in(kv)))\n",
    "\n",
    "        if ENABLE_FLASH_ATTN:\n",
    "            qkv = flash_attn_func(q, k, v) # flash attention not on TPU\n",
    "            concatted = qkv.reshape(B, L, E) # M N H D -> M N (H D)\n",
    "        else:\n",
    "            qkv = F.scaled_dot_product_attention(q, k, v) # flash attention not on TPU\n",
    "            concatted = qkv.permute(0, 2, 1, 3).reshape(B, L, E).contiguous() # M H N D -> M N (H D)\n",
    "        return self.out(concatted)\n",
    "    \n",
    "class Attn2d(nn.Module):\n",
    "    def __init__(self, nc: int):\n",
    "        '''\n",
    "        nc: number of input and output channels\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.nc = nc\n",
    "        self.norm = StableNorm(32, self.nc)\n",
    "        self.attn = MHA(self.nc, max(self.nc // 128, 1)) # max head dim is 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x).reshape(B, C, H*W).permute(0, 2, 1) # B C H W -> B (H W) C\n",
    "        h = self.attn(h)\n",
    "        h = h.permute(0, 2, 1).reshape(B, C, H, W) # B (H W) C -> B C H W\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.047890Z",
     "iopub.status.busy": "2023-09-16T15:02:13.047594Z",
     "iopub.status.idle": "2023-09-16T15:02:13.063756Z",
     "shell.execute_reply": "2023-09-16T15:02:13.062785Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.047865Z"
    }
   },
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, in_c: int, nc: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_c, nc, bias=bias)\n",
    "        self.gate = nn.Linear(in_c, nc, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x) * F.silu(self.gate(x))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''\n",
    "    Attention between UNet feature maps and text embeddings.\n",
    "    '''\n",
    "    def __init__(self, d: int, d_emb: int, nh: int = 8):\n",
    "        '''\n",
    "        d: attention dimensionality\n",
    "        d_emb: dimensionality of conditioning embedding (text conditioning)\n",
    "        nh: number of heads\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.norm3 = nn.LayerNorm(d)\n",
    "        self.attn1 = MHA(d, nh)\n",
    "        self.attn2 = MHA(d, nh, kv_dim=d_emb)\n",
    "        self.ff = nn.Sequential(\n",
    "            SwiGLU(d, 4*d, bias=False),\n",
    "            zero_module(nn.Linear(4*d, d, bias=False)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        B, C, H, W = x.shape\n",
    "        skip = x\n",
    "        x = x.reshape(B, C, H*W).permute(0, 2, 1).contiguous() # B C H W -> B (H W) C\n",
    "        x = self.attn1(self.norm1(x)) + x\n",
    "        if context is not None:\n",
    "            x = self.attn2(self.norm2(x), context) + x\n",
    "        x = self.ff(self.norm3(x)) + x\n",
    "        x = x.permute(0, 2, 1).reshape(B, C, H, W).contiguous() # B (H W) C -> B C H W\n",
    "\n",
    "        return x + skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestep Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.065421Z",
     "iopub.status.busy": "2023-09-16T15:02:13.065119Z",
     "iopub.status.idle": "2023-09-16T15:02:13.088886Z",
     "shell.execute_reply": "2023-09-16T15:02:13.088013Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.065394Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    '''\n",
    "    Sinusoidal time embedding with a feed forward network.\n",
    "    '''\n",
    "    def __init__(self, embed_c: int, out_c: int, max_period: int = 10000):\n",
    "        '''\n",
    "        embed_c: dimensionality of sinusoidal time embedding\n",
    "        out_c: dimensionality of projected (output) embedding\n",
    "        max_period: controls the minimum frequency of the embeddings\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.embed_c = embed_c\n",
    "        self.out_c = out_c\n",
    "        self.max_period = max_period\n",
    "        half = embed_c // 2\n",
    "        self.register_buffer('freqs', torch.exp(-math.log(max_period) * torch.linspace(0, 1, half, dtype=torch.double)))\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_c, out_c),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_c, out_c),\n",
    "        )\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        t_freqs = timesteps[:, None] * self.freqs[None, :]\n",
    "        emb = torch.cat([t_freqs.cos(), t_freqs.sin()], dim=-1).float()\n",
    "        emb = self.ff(emb)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfcd59fb-1e30-42ad-a8ac-cfcc8584d7b3",
    "_uuid": "3ec6ac05-a924-4c85-8887-645c05d03aea"
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf60fbf0-54e4-4f4c-9513-0b32ff161576",
    "_uuid": "d1c63cab-e6f8-4227-8d67-6c200cfb4fd7"
   },
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df38f4b3-309e-4172-9f1e-a17a39466094",
    "_uuid": "a378c32d-94a0-451d-9567-e4fdde69dfab",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.090342Z",
     "iopub.status.busy": "2023-09-16T15:02:13.090047Z",
     "iopub.status.idle": "2023-09-16T15:02:13.119548Z",
     "shell.execute_reply": "2023-09-16T15:02:13.118412Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.090316Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "            nc: int,\n",
    "            ch_mults: tuple,\n",
    "            nlayers_per_res: int,\n",
    "            nz: int\n",
    "            ):\n",
    "        '''\n",
    "        nc: base number of channels\n",
    "        ch_mults: channel multiplier per resolution\n",
    "        nz: number of latent output channels\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # put input args as class fields\n",
    "        locals_ = locals().copy()\n",
    "        locals_.pop('self')\n",
    "        for k, v in locals_.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        layers = [nn.Conv2d(3, nc, 3, padding=1)]\n",
    "        out_c = nc\n",
    "\n",
    "        for layer_idx, ch_mult in enumerate(ch_mults):\n",
    "            in_c, out_c = out_c, nc * ch_mult\n",
    "            layers += [ResBlock(in_c, out_c)] + [ResBlock(out_c, out_c) for _ in range(nlayers_per_res-1)]\n",
    "            if layer_idx != len(ch_mults) - 1:\n",
    "                layers.append(Downsample(out_c))\n",
    "\n",
    "        layers += [ \n",
    "            # mid\n",
    "            ResBlock(out_c, out_c),\n",
    "            Attn2d(out_c),\n",
    "            ResBlock(out_c, out_c),\n",
    "\n",
    "            # out\n",
    "            StableNorm(32, out_c),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_c, 2*nz, 3, padding=1),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x) # nz*2 for the mean, stdev\n",
    "        return x\n",
    "\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "            nc: int,\n",
    "            ch_mults: tuple,\n",
    "            nlayers_per_res: int,\n",
    "            nz: int\n",
    "            ):\n",
    "        '''\n",
    "        nc: base number of channels\n",
    "        ch_mults: channel multiplier per resolution\n",
    "        nz: number of latent output channels\n",
    "        '''\n",
    "        super().__init__()\n",
    "        # put input args as class fields\n",
    "        locals_ = locals().copy()\n",
    "        locals_.pop('self')\n",
    "        for k, v in locals_.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        out_c = self.nc * self.ch_mults[-1] \n",
    "        layers = [\n",
    "            nn.Conv2d(nz, out_c, 3, padding=1),\n",
    "            ResBlock(out_c, out_c),\n",
    "            Attn2d(out_c),\n",
    "            ResBlock(out_c, out_c),\n",
    "        ]\n",
    "\n",
    "        for layer_idx, ch_mult in enumerate(reversed(ch_mults)):\n",
    "            in_c, out_c = out_c, self.nc * ch_mult\n",
    "            layers += [ResBlock(in_c, out_c)] + [ResBlock(out_c, out_c) for _ in range(nlayers_per_res-1)]\n",
    "            if layer_idx != 0:\n",
    "                layers.append(Upsample(out_c))\n",
    "\n",
    "        layers += [\n",
    "            StableNorm(32, out_c),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_c, 3, 3, padding=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nc: int = VAE_NC,\n",
    "        ch_mults: list = VAE_CH_MULTS,\n",
    "        nlayers_per_res: int = VAE_NLAYERS_PER_RES,\n",
    "        nz: int = VAE_NZ\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # put input args as class fields\n",
    "        locals_ = locals().copy()\n",
    "        locals_.pop('self')\n",
    "        for k, v in locals_.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.encoder = VAEEncoder(nc=self.nc, ch_mults=self.ch_mults, nlayers_per_res=self.nlayers_per_res, nz=self.nz)\n",
    "        self.decoder = VAEDecoder(nc=self.nc, ch_mults=self.ch_mults, nlayers_per_res=self.nlayers_per_res, nz=self.nz)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_params = self.encoder(x)\n",
    "        mean, log_var = torch.split(z_params, self.nz, dim=1)\n",
    "        z = self.sample(mean, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mean, log_var\n",
    "\n",
    "    def sample(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(log_var)\n",
    "        return mean + eps * std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIP Text Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.124005Z",
     "iopub.status.busy": "2023-09-16T15:02:13.123682Z",
     "iopub.status.idle": "2023-09-16T15:02:13.136375Z",
     "shell.execute_reply": "2023-09-16T15:02:13.135458Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.123962Z"
    }
   },
   "outputs": [],
   "source": [
    "class FrozenCLIPEmbedder(nn.Module):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=CLIP_VERSION, max_length=MAX_TOKENS):\n",
    "        super().__init__()\n",
    "        from transformers import CLIPTokenizerFast as CLIPTokenizer, CLIPTextModel\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.device = 'cpu'\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        return super().to(device)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text=None, tokens=None):\n",
    "        assert not (tokens is None and text is None)\n",
    "        if tokens is None:\n",
    "            batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                            return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "            tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        outputs = self.transformer(input_ids=tokens)\n",
    "\n",
    "        z = outputs.last_hidden_state\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Timestep Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.137678Z",
     "iopub.status.busy": "2023-09-16T15:02:13.137396Z",
     "iopub.status.idle": "2023-09-16T15:02:13.160826Z",
     "shell.execute_reply": "2023-09-16T15:02:13.159882Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.137653Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimestepEmbedSequential(nn.Sequential):\n",
    "    '''\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    '''\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                x = layer(x, emb)\n",
    "            elif isinstance(layer, TransformerBlock):\n",
    "                x = layer(x, context)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.162426Z",
     "iopub.status.busy": "2023-09-16T15:02:13.162109Z",
     "iopub.status.idle": "2023-09-16T15:02:13.185950Z",
     "shell.execute_reply": "2023-09-16T15:02:13.185106Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.162399Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "        in_c: int = DDPM_NZ,\n",
    "        nc: int = DDPM_NC,\n",
    "        ch_mults: list = DDPM_CH_MULTS,\n",
    "        attn_resolutions: list = ATTN_RESOLUTIONS,\n",
    "        nlayers_per_res: int = DDPM_NLAYERS_PER_RES,\n",
    "        context_dim: int = TEXT_CONTEXT_DIM,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # put input args as class fields\n",
    "        locals_ = locals().copy()\n",
    "        locals_.pop('self')\n",
    "        for k, v in locals_.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        temb_c = nc * 4\n",
    "        self.time_embed = TimeEmbedding(nc, temb_c)\n",
    "\n",
    "        # adding downsampling blocks\n",
    "        out_c = nc\n",
    "        self.downs = nn.ModuleList([TimestepEmbedSequential(nn.Conv2d(in_c, nc, 3, padding=1))])\n",
    "        down_out_cs = [nc] # output channel counts of the downsampling activation stack\n",
    "        for block_idx, ch_mult in enumerate(ch_mults):\n",
    "            for n_idx in range(nlayers_per_res):\n",
    "                in_c, out_c = out_c, nc * ch_mult\n",
    "\n",
    "                block = [ResBlock(in_c, out_c, temb_c)] + [TransformerBlock(out_c, context_dim) for _ in range(attn_resolutions[block_idx])]\n",
    "\n",
    "                block = TimestepEmbedSequential(*block)\n",
    "                self.downs.append(block)\n",
    "                down_out_cs.append(out_c)\n",
    "\n",
    "            if block_idx != len(ch_mults) - 1:\n",
    "                self.downs.append(TimestepEmbedSequential(Downsample(out_c)))\n",
    "                down_out_cs.append(out_c)\n",
    "\n",
    "        # middle block\n",
    "        n_mid_transformer_blocks = max(1, attn_resolutions[-1])\n",
    "        self.mid_block = TimestepEmbedSequential(\n",
    "                ResBlock(out_c, out_c, temb_c),\n",
    "                *[TransformerBlock(out_c, context_dim) for _ in range(n_mid_transformer_blocks)],\n",
    "                ResBlock(out_c, out_c, temb_c),\n",
    "            )\n",
    "\n",
    "        # adding upsampling blocks\n",
    "        self.ups = nn.ModuleList([])\n",
    "        for block_idx, ch_mult in enumerate(reversed(ch_mults)):\n",
    "            for n_idx in range(nlayers_per_res+1):\n",
    "                down_c = down_out_cs.pop()\n",
    "                in_c, out_c = out_c + down_c, nc * ch_mult\n",
    "\n",
    "                block = [ResBlock(in_c, out_c, temb_c)] + [TransformerBlock(out_c, context_dim) for _ in range(attn_resolutions[-block_idx-1])]\n",
    "\n",
    "                if n_idx == nlayers_per_res and block_idx != len(ch_mults)-1:\n",
    "                    block.append(Upsample(out_c))\n",
    "\n",
    "                block = TimestepEmbedSequential(*block)\n",
    "                self.ups.append(block)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            StableNorm(32, nc),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(nc, self.in_c, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps, context=None):\n",
    "        temb = self.time_embed(timesteps)\n",
    "        downs = []\n",
    "        for block in self.downs:\n",
    "            x = block(x, temb, context)\n",
    "            downs.append(x)\n",
    "        x = self.mid_block(x, temb, context)\n",
    "        for block in self.ups:\n",
    "            x = torch.cat([x, downs.pop()], dim=1)\n",
    "            x = block(x, temb, context)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.187408Z",
     "iopub.status.busy": "2023-09-16T15:02:13.187108Z",
     "iopub.status.idle": "2023-09-16T15:02:13.204961Z",
     "shell.execute_reply": "2023-09-16T15:02:13.204100Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.187381Z"
    }
   },
   "outputs": [],
   "source": [
    "class NoiseSchedule(nn.Module):\n",
    "    def __init__(self, t=1000, beta_min=8.5e-4, beta_max=1.2e-2):\n",
    "        super().__init__()\n",
    "        self.t = torch.ones((), dtype=torch.int) * t\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.register_buffer('betas', torch.linspace(beta_min**0.5, beta_max**0.5, t, dtype=torch.double) ** 2)\n",
    "        self.register_buffer('alphas', 1 - self.betas)\n",
    "        self.register_buffer('alpha_prods', torch.cumprod(self.alphas, dim=0))\n",
    "        self.register_buffer('signal_stds', self.alpha_prods.sqrt())\n",
    "        self.register_buffer('noise_stds', (1 - self.alpha_prods).sqrt())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # noises each sample in x to a random timestep \n",
    "        B = x.shape[0]\n",
    "        noise = torch.randn_like(x)\n",
    "        timesteps = torch.randint(self.t, (B,), device=x.device)\n",
    "        noised = self.signal_stds[timesteps][:, None, None, None] * x.double() + self.noise_stds[timesteps][:, None, None, None] * noise.double()\n",
    "        return noised.float(), timesteps.long(), noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDIM Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.206612Z",
     "iopub.status.busy": "2023-09-16T15:02:13.206218Z",
     "iopub.status.idle": "2023-09-16T15:02:13.231890Z",
     "shell.execute_reply": "2023-09-16T15:02:13.230940Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.206583Z"
    }
   },
   "outputs": [],
   "source": [
    "class DDIMSampler():\n",
    "    def __init__(self, denoiser,\n",
    "            noise_schedule=NoiseSchedule(),\n",
    "            tau_dim=None,\n",
    "            eta=0.0,\n",
    "        ):\n",
    "        device = noise_schedule.alpha_prods.device\n",
    "        self.denoiser = denoiser\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.tau_dim = noise_schedule.t if tau_dim is None else tau_dim\n",
    "        self.tau = np.linspace(0, self.noise_schedule.t-1, self.tau_dim).astype(np.int32)\n",
    "        self.eta = torch.ones((), device=device, dtype=torch.double) * eta\n",
    "\n",
    "        self.alphas = torch.cat([torch.ones((1,), dtype=torch.double).to(device), self.noise_schedule.alpha_prods[self.tau]])\n",
    "        self.betas = 1 - self.alphas\n",
    "\n",
    "    def denoise_step(self, x, t, noise_pred):\n",
    "        beta_ratio = self.betas[t-1] / self.betas[t]\n",
    "        alpha_ratio = self.alphas[t] / self.alphas[t-1]\n",
    "        sigma = self.eta * (beta_ratio * (1 - alpha_ratio))**0.5\n",
    "        x0_step = self.alphas[t]**-0.5 * (x - self.betas[t]**0.5 * noise_pred)\n",
    "        xt_step = (1 - self.alphas[t-1] - sigma**2)**0.5 * noise_pred\n",
    "        added_noise = sigma * torch.randn_like(x)\n",
    "        return self.alphas[t-1]**0.5 * x0_step + xt_step + added_noise\n",
    "\n",
    "    def get_samples(self, initial_x=None, x_shape=None, context=None, verbose=True, cfg_weight=5):\n",
    "        if initial_x is None and x_shape is None:\n",
    "            raise Exception('Either initial_x or x_shape must be defined.')\n",
    "\n",
    "        x = torch.randn(x_shape, dtype=torch.double) if initial_x is None else initial_x.double()\n",
    "        \n",
    "        pbar = range(self.tau_dim, 0, -1)\n",
    "        if xm.is_master_ordinal():\n",
    "            pbar = tqdm(pbar, position=0, leave=True)\n",
    "            \n",
    "        for t in pbar:\n",
    "            t = torch.Tensor((t,)).long().to(x.device)\n",
    "            t_repeated = self.tau[t-1] * torch.ones(x.shape[:1], dtype=int).to(x.device)\n",
    "            if cfg_weight in (0, None):\n",
    "                noise_pred = self.denoiser(x.float(), t_repeated, context).double()\n",
    "            else:\n",
    "                noise_pred = (1 + cfg_weight) * self.denoiser(x.float(), t_repeated, context).double() - cfg_weight * self.denoiser(x.float(), t_repeated, torch.zeros_like(context)).double()\n",
    "            x = self.denoise_step(x, t, noise_pred)\n",
    "            xm.mark_step()\n",
    "        return x.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4996d1b6-56ae-49d1-9131-30d100ef5931",
    "_uuid": "0b2ce698-fde4-48b5-81c9-885238d0f5c1"
   },
   "source": [
    "Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e43a5b62-d6c3-4fc0-a777-484d01eb1ebc",
    "_uuid": "acc9511f-18f3-4dbc-b0d1-af4b9738e7ff",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.233433Z",
     "iopub.status.busy": "2023-09-16T15:02:13.233110Z",
     "iopub.status.idle": "2023-09-16T15:02:13.247436Z",
     "shell.execute_reply": "2023-09-16T15:02:13.246577Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.233406Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_models(device='cpu', lr=1e-4, lr_warmup_steps=LR_WARMUP_STEPS, ckpt_path=None, vae_ckpt_path=None):\n",
    "    ddpm = UNet().to(device)\n",
    "    vae = VAE().eval().to(device)\n",
    "    for param in vae.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    text_embedder = FrozenCLIPEmbedder().to(device)\n",
    "    text_embedder.transformer = torch.compile(text_embedder.transformer, backend='torchxla_trace_once')\n",
    "    noise_schedule = NoiseSchedule().to(device)\n",
    "    sampler = DDIMSampler(ddpm, noise_schedule=noise_schedule, tau_dim=200)\n",
    "    opt = AdamW(ddpm.parameters(), lr)\n",
    "    scheduler = LambdaLR(\n",
    "        opt,\n",
    "        lambda step: min(step, lr_warmup_steps)*(1-1e-6)/lr_warmup_steps + 1e-6\n",
    "    )\n",
    "    \n",
    "    epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    if vae_ckpt_path is not None:\n",
    "        xm.master_print(f'Loading VAE checkpoint from {vae_ckpt_path}')\n",
    "        vae_ckpt = torch.load(vae_ckpt_path, map_location=torch.device('cpu'))\n",
    "        vae.load_state_dict(vae_ckpt['vae_state_dict'])\n",
    "\n",
    "    if ckpt_path is not None:\n",
    "        xm.master_print(f'Loading checkpoint from {ckpt_path}')\n",
    "        ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "        ddpm.load_state_dict(ckpt['ddpm_state_dict'])\n",
    "        ddpm = ddpm.to(device)\n",
    "        opt = AdamW(ddpm.parameters(), lr)\n",
    "        opt.load_state_dict(ckpt['opt_state_dict'])\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "        epoch = ckpt['epoch']\n",
    "        global_step = ckpt['global_step']\n",
    "\n",
    "    return {\n",
    "            'ddpm': ddpm, 'vae': vae, 'text_embedder': text_embedder,\n",
    "            'noise_schedule': noise_schedule, 'sampler': sampler,\n",
    "            'opt': opt, 'scheduler': scheduler,\n",
    "            'epoch': epoch, 'global_step': global_step\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7de8ee74-247f-4821-a1f4-45cabe790caa",
    "_uuid": "fe18232f-5ff7-4a2c-87b9-2e7fc744eb60"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0949667c-fc54-491f-878b-eb931fba93f6",
    "_uuid": "2e8d67b4-045a-4eec-8a54-0d95887c9149",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.249119Z",
     "iopub.status.busy": "2023-09-16T15:02:13.248703Z",
     "iopub.status.idle": "2023-09-16T15:02:13.289567Z",
     "shell.execute_reply": "2023-09-16T15:02:13.288693Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.249089Z"
    }
   },
   "outputs": [],
   "source": [
    "class DDPMTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ddpm, vae, text_embedder,\n",
    "        noise_schedule, sampler,\n",
    "        loader,\n",
    "        opt, scheduler,\n",
    "        img_size, ckpt_save_dir,\n",
    "        wandb_run, device,\n",
    "        global_step=0\n",
    "    ):\n",
    "        # put input args as class fields\n",
    "        locals_ = locals().copy()\n",
    "        locals_.pop('self')\n",
    "        for k, v in locals_.items():\n",
    "            setattr(self, k, v)\n",
    "        self.p_uncond = 0.1\n",
    "        self.z_std = None\n",
    "        self.model_fnames = []\n",
    "\n",
    "        vae_downscale_factor = int(2 ** (len(self.vae.ch_mults) - 1))\n",
    "        if isinstance(img_size, int):\n",
    "            z_spatial = img_size // vae_downscale_factor\n",
    "            self.z_shape = (self.vae.nz, z_spatial, z_spatial)\n",
    "        else:\n",
    "            z_h = img_size[0] // vae_downscale_factor\n",
    "            z_w = img_size[1] // vae_downscale_factor\n",
    "            self.z_shape = (self.vae.nz, z_h, z_w)\n",
    "            \n",
    "    def display_predictions(self, n_rows=2, n_cols=4,\n",
    "            captions=None, display=True, save_path=None):\n",
    "        if not xm.is_master_ordinal():\n",
    "            xm.rendezvous('display')\n",
    "            return\n",
    "        \n",
    "        n_imgs = n_rows * n_cols\n",
    "        \n",
    "        if captions is None:\n",
    "            # get random captions from the training dataset\n",
    "            captions = next(iter(self.loader))[1][:n_imgs]\n",
    "        text_embeddings = self.text_embedder(captions)\n",
    "\n",
    "        z = torch.randn((n_imgs, *self.z_shape), device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_denoised = self.sampler.get_samples(initial_x=z, context=text_embeddings)\n",
    "            z_std = 1 if self.z_std is None else self.z_std\n",
    "            preds = self.vae.decoder(z_denoised * z_std)\n",
    "\n",
    "        preds = preds.permute(0, 2, 3, 1).detach().float().cpu().clamp(-1, 1) * 0.5 + 0.5\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=(20, 12))\n",
    "        \n",
    "        word_wrap = lambda text: '\\n'.join(wrap(text, 30))\n",
    "\n",
    "        for i in range(n_imgs):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            ax[row][col].title.set_text(word_wrap(captions[i]))\n",
    "            ax[row][col].imshow(preds[i])\n",
    "\n",
    "        if save_path is not None:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            fig.savefig(save_path)\n",
    "\n",
    "        if self.wandb_run is not None:\n",
    "            wandb.log({f'preds': wandb.Image(fig)})\n",
    "\n",
    "        if display:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close('all')\n",
    "        xm.rendezvous('display')\n",
    "\n",
    "    def save_models(self, epoch=0, remove_after_save=True):\n",
    "        ckpt_save_path = f'{self.ckpt_save_dir}/ddpm_{epoch:0>4}.pth'\n",
    "        xm.mark_step()\n",
    "        xm.save({\n",
    "            'ddpm_state_dict': self.ddpm.state_dict(),\n",
    "            'opt_state_dict': self.opt.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'global_step': self.global_step,\n",
    "            }, ckpt_save_path)\n",
    "\n",
    "        if self.wandb_run is not None:\n",
    "            wandb.save(ckpt_save_path)\n",
    "            self.model_fnames.append(ckpt_save_path)\n",
    "            if remove_after_save and len(self.model_fnames) >= 2:\n",
    "                os.remove(self.model_fnames.pop(0))\n",
    "\n",
    "    def train_step(self, imgs, captions):\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "        text_embeddings = self.text_embedder(captions)\n",
    "        cond_mask = torch.where(torch.rand((text_embeddings.shape[0], 1, 1), device=self.device) < self.p_uncond, 0.0, 1.0)\n",
    "        text_embeddings = text_embeddings * cond_mask\n",
    "\n",
    "        with xp.Trace('enc'):\n",
    "            z_params = self.vae.encoder(imgs)\n",
    "            mean, log_var = torch.split(z_params, self.vae.nz, dim=1)\n",
    "            z = self.vae.sample(mean, log_var)\n",
    "            if self.z_std is None:\n",
    "                self.z_std = xm.all_reduce('sum', z.std(), scale=0.125).detach()\n",
    "                xm.master_print(f'local z_std: {z.std()}, global z_std: {self.z_std}')\n",
    "            z = z / self.z_std\n",
    "\n",
    "        with xp.Trace('noise'):\n",
    "            noised, timesteps, noise = self.noise_schedule(z)\n",
    "            \n",
    "        with xp.Trace('forward'):\n",
    "            noise_preds = self.ddpm(noised.detach(), timesteps.detach(), text_embeddings.detach())\n",
    "            loss = F.mse_loss(noise_preds, noise)\n",
    "            \n",
    "        with xp.Trace('backward'):\n",
    "            loss.backward()\n",
    "        \n",
    "        with xp.Trace('opt_step'):\n",
    "            xm.optimizer_step(self.opt)\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        return {'noise_loss': loss}\n",
    "    \n",
    "    def log_step(self,\n",
    "        log_metrics: dict, step: int,\n",
    "        epoch: int, epochs: int, steps_per_epoch: int, pbar\n",
    "    ):\n",
    "        log_items = {k: xm.mesh_reduce('sum', v.item(), lambda vals: sum(vals) / 8) for k, v in log_metrics.items()} # .item() must be executed out of is_master_ordinal() or will hang\n",
    "        if xm.is_master_ordinal():\n",
    "            pbar.update(STEPS_PER_LOG)\n",
    "            pbar.set_description(f'Epoch {epoch+1}/{epochs}, {(step+1) % steps_per_epoch}/{steps_per_epoch}')\n",
    "            \n",
    "            if self.wandb_run is not None:\n",
    "                wandb.log(log_items, step=self.global_step)\n",
    "\n",
    "    def train_loop(self, epochs=1, steps_per_epoch=-1,\n",
    "            save_every_n_epochs=1, epoch=0,\n",
    "            global_step=0):\n",
    "        loader = pl.MpDeviceLoader(self.loader, self.device)\n",
    "        gc.collect()\n",
    "        pbar = tqdm(total=steps_per_epoch) if xm.is_master_ordinal() else None\n",
    "        for step, (imgs, captions) in enumerate(loader):\n",
    "            log_metrics = self.train_step(imgs, captions)\n",
    "\n",
    "            if (step + 1) % STEPS_PER_LOG == 0:\n",
    "                self.global_step += STEPS_PER_LOG\n",
    "                xm.add_step_closure(self.log_step, args=(\n",
    "                    log_metrics, step,\n",
    "                    epoch, epochs, steps_per_epoch, pbar\n",
    "                ))\n",
    "                \n",
    "            if (step + 1) % steps_per_epoch == 0:\n",
    "                epoch += 1\n",
    "                if epoch % save_every_n_epochs == 0:\n",
    "                    self.save_models(epoch,)\n",
    "                    self.display_predictions(2, 4, None, True, f'predictions/out_{epoch:0>4}.jpg',)\n",
    "                if xm.is_master_ordinal():\n",
    "                    pbar.close()\n",
    "                    pbar = tqdm(total=steps_per_epoch)\n",
    "                if epoch == epochs:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb5d66c8-de04-4c31-8ae9-3fd0821c97bf",
    "_uuid": "72c403f0-d245-4cdd-b287-426e243e6db9"
   },
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "685ae9ff-9da3-4ca5-bfb3-d2c31d0efbf4",
    "_uuid": "fb7f0550-c345-4437-98c7-0749def6e9e9",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.291193Z",
     "iopub.status.busy": "2023-09-16T15:02:13.290775Z",
     "iopub.status.idle": "2023-09-16T15:02:13.309189Z",
     "shell.execute_reply": "2023-09-16T15:02:13.308311Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.291168Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    rank: int, seeds: list,\n",
    "    epochs: int, steps_per_epoch: int,\n",
    "    crop_size: int, batch_size: int,\n",
    "    learning_rate: float, lr_warmup_steps: int,\n",
    "    ckpt_save_dir: str, ckpt_path: str = None,\n",
    "    vae_ckpt_path: str = None,\n",
    "):\n",
    "    device = xm.xla_device()\n",
    "    server = xp.start_server(6000)\n",
    "    dist.init_process_group('xla', init_method='pjrt://')\n",
    "    \n",
    "    xm.set_rng_state(seeds[rank])\n",
    "    np.random.seed(seeds[rank])\n",
    "    \n",
    "    n_devices = 8\n",
    "    s_i = rank * len(FF_CHILD) // n_devices\n",
    "    e_i = len(FF_CHILD) if rank == n_devices - 1 else (rank + 1) * len(FF_CHILD) // n_devices\n",
    "    splits = {\n",
    "        f: 1 / (e_i - s_i) for f in FF_CHILD[s_i:e_i] # uniform sampling\n",
    "    }\n",
    "    dataset = MultiTFRecordDataset(TFREC_PATTERN, INDEX_PATTERN, splits, transform=process_ex, shuffle_queue_size=8192)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    loaded = load_models(\n",
    "            device=device, lr=learning_rate, lr_warmup_steps=lr_warmup_steps,\n",
    "            ckpt_path=ckpt_path, vae_ckpt_path=vae_ckpt_path)\n",
    "    ddpm, vae, text_embedder = loaded['ddpm'], loaded['vae'], loaded['text_embedder']\n",
    "    noise_schedule, sampler = loaded['noise_schedule'], loaded['sampler']\n",
    "    opt, scheduler = loaded['opt'], loaded['scheduler']\n",
    "    epoch, global_step = loaded['epoch'], loaded['global_step']\n",
    "    \n",
    "    get_nparams = lambda model: sum(p.numel() for p in model.parameters())\n",
    "    xm.master_print(f'Models loaded, DDPM nparams: {get_nparams(ddpm)}, VAE nparams: {get_nparams(vae)}')\n",
    "    \n",
    "    if global_step == 0:\n",
    "        pjrt.broadcast_master_param(ddpm)\n",
    "        \n",
    "    xm.master_print('Model params broadcasted.')\n",
    "\n",
    "    wandb_run = None\n",
    "    if USE_WANDB and rank == 0:\n",
    "        wandb_run = wandb.init(project='tpu_ldm_ddpm_v2')\n",
    "        wandb.save('/kaggle/working/train.py')\n",
    "\n",
    "    trainer = DDPMTrainer(\n",
    "        ddpm, vae, text_embedder,\n",
    "        noise_schedule, sampler,\n",
    "        loader,\n",
    "        opt, scheduler,\n",
    "        crop_size, ckpt_save_dir,\n",
    "        wandb_run, device,\n",
    "        global_step=global_step,\n",
    "    )\n",
    "    \n",
    "    xm.master_print('Trainer made.')\n",
    "    \n",
    "    del ddpm, vae, text_embedder, noise_schedule, opt # these are in the device so can be safely deleted from cpu mem\n",
    "    gc.collect()\n",
    "\n",
    "    if global_step == 0 and not DEBUG:\n",
    "        trainer.display_predictions(save_path=f'predictions/out_initial.jpg')\n",
    "        print(rank)\n",
    "    trainer.train_loop(epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "        epoch=epoch, global_step=global_step)\n",
    "\n",
    "    if USE_WANDB and rank == 0:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:13.310692Z",
     "iopub.status.busy": "2023-09-16T15:02:13.310389Z",
     "iopub.status.idle": "2023-09-16T15:02:32.063174Z",
     "shell.execute_reply": "2023-09-16T15:02:32.062151Z",
     "shell.execute_reply.started": "2023-09-16T15:02:13.310665Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_WANDB and WANDB_RUN_PATH is not None: # load DDPM checkpoint from wandb\n",
    "    wandb.restore(WANDB_MODEL_FNAME, run_path=WANDB_RUN_PATH, root=CKPT_DIR)\n",
    "if USE_WANDB and WANDB_VAE_RUN_PATH is not None: # load VAE checkpoint from wandb\n",
    "    wandb.restore(WANDB_VAE_MODEL_FNAME, run_path=WANDB_VAE_RUN_PATH, root=CKPT_DIR)\n",
    "\n",
    "# get most recent VAE checkpoint for resuming training\n",
    "VAE_CKPT_PATH = sorted(glob.glob(f'{CKPT_DIR}/vae_*.pth'))\n",
    "if VAE_CKPT_PATH != []:\n",
    "    VAE_CKPT_PATH = VAE_CKPT_PATH[-1]\n",
    "else:\n",
    "    VAE_CKPT_PATH = None\n",
    "    print('Warning: no pretrained VAE found.')\n",
    "\n",
    "# get most recent DDPM checkpoint for resuming training\n",
    "CKPT_PATH = sorted(glob.glob(f'{CKPT_DIR}/ddpm_*.pth'))\n",
    "if CKPT_PATH != []:\n",
    "    CKPT_PATH = CKPT_PATH[-1]\n",
    "else:\n",
    "    CKPT_PATH = None\n",
    "os.makedirs(CKPT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "train_args = (\n",
    "    torch.randint(0, 2**32, (8,)),\n",
    "    EPOCHS, STEPS_PER_EPOCH,\n",
    "    CROP_SIZE, BATCH_SIZE,\n",
    "    LEARNING_RATE, LR_WARMUP_STEPS,\n",
    "    CKPT_SAVE_DIR, CKPT_PATH,\n",
    "    VAE_CKPT_PATH\n",
    ")\n",
    "\n",
    "# commented so training command can be shown without running\n",
    "#if ONE_CORE:\n",
    "#    train(0, *train_args)\n",
    "#else:\n",
    "#    xmp.spawn(train, args=train_args, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1520d5e-7a9e-4124-a72f-6e1a3eb16d06",
    "_uuid": "35233ae8-8b52-4aca-9e91-bf2371b78969",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:32.065019Z",
     "iopub.status.busy": "2023-09-16T15:02:32.064566Z",
     "iopub.status.idle": "2023-09-16T15:02:33.207690Z",
     "shell.execute_reply": "2023-09-16T15:02:33.206390Z",
     "shell.execute_reply.started": "2023-09-16T15:02:32.064986Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm /kaggle/working/train.py\n",
    "%history -f /kaggle/working/train.py -l 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c356ba4-e0c8-4671-a040-d9c501f1d1bd",
    "_uuid": "9aaecaf1-c1a4-45b7-a23f-4770b072c95f"
   },
   "source": [
    "Run Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c67d94e6-0aeb-47cb-876d-90ef3e01a7af",
    "_uuid": "882cb759-18a4-44e2-8e87-7331b77cdbfe",
    "execution": {
     "iopub.execute_input": "2023-09-16T15:02:33.209602Z",
     "iopub.status.busy": "2023-09-16T15:02:33.209255Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ONE_CORE:\n",
    "    print('Single-core training')\n",
    "    train(0, *train_args)\n",
    "else:\n",
    "    print('Multi TPU core training')\n",
    "    xmp.spawn(train, args=train_args, start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r logs *pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
